{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hmmlearn pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hmm\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from warnings import filterwarnings\n",
    "import matplotlib.pyplot as plt\n",
    "import hmmlearn.hmm as hmm\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ------------------- Configuration -------------------\n",
    "SYMBOL = 'TSLA'\n",
    "folder = \"TSLA\"\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2025-06-27'\n",
    "PERIOD = \"1d\"\n",
    "WINDOW_SIZE = 20\n",
    "TRADING_FEES = 0.002\n",
    "LOOKBACKS = [1, 5, 10, 21]\n",
    "\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "\n",
    "data = yf.Ticker(SYMBOL).history(start=START_DATE, end=END_DATE)\n",
    "data = data.drop([\"Dividends\", \"Stock Splits\"], axis = 1)\n",
    "\n",
    "\n",
    "total_size = len(data)\n",
    "train_size = int(total_size * 0.7)\n",
    "gap_size = max(int(total_size * 0.1), 24)\n",
    "test_size = total_size - train_size - gap_size\n",
    "\n",
    "if test_size <= 30:\n",
    "    raise ValueError(\"Insufficient data for meaningful backtesting\")\n",
    "\n",
    "train_data = data.iloc[:train_size].copy()\n",
    "test_data = data.iloc[train_size + gap_size:].copy()\n",
    "\n",
    "def create_features(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    for days in LOOKBACKS:\n",
    "      data[f\"Log_Return_{days}\"] = np.log(data['Close'] / data['Close'].shift(days))      \n",
    "      if (LOOKBACKS.index(days)+1) % 2 == 0:\n",
    "        data[f'Volatility_{days}'] = data[f\"Log_Return_{days}\"].rolling(days, min_periods=1).std()\n",
    "      if days >= 10:\n",
    "        data[f\"Momentum_{days}\"] = data[\"Close\"].shift(1) - data[\"Close\"].shift(days + 1)\n",
    "    \n",
    "    log_volume = np.log(data[\"Volume\"].shift(1) + 1e-6)\n",
    "    rolling_mean = log_volume.rolling(window=5, min_periods=5).mean()\n",
    "    rolling_std = log_volume.rolling(window=5, min_periods=5).std()\n",
    "    data[\"Z_Log_Volume\"] = (log_volume - rolling_mean) / (rolling_std + 1e-6)\n",
    "\n",
    "    MA = data[\"Close\"].rolling(WINDOW_SIZE).mean()\n",
    "    STD = data[\"Close\"].rolling(WINDOW_SIZE).std()\n",
    "    data[\"Z_Price_vs_MA\"] = (data[\"Close\"] - MA) / (STD + 1e-6)\n",
    "\n",
    "\n",
    "    return data.dropna()\n",
    "\n",
    "train_data = create_features(train_data)\n",
    "test_data = create_features(test_data)\n",
    "feature_cols = [col for col in train_data.columns if any(key in col for key in ['Log_Return', 'Volatility', 'Momentum', 'Z_'])]\n",
    "X_train = train_data[feature_cols]\n",
    "X_test = train_data[feature_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "hmm_model = hmm.GaussianHMM(\n",
    "    n_components=5,\n",
    "    covariance_type=\"full\",\n",
    "    n_iter=400,\n",
    "    tol=1e-4,\n",
    "    #random_state=42,\n",
    "    verbose=True,\n",
    "    init_params=\"stmc\"\n",
    ")\n",
    "\n",
    "hmm_model.fit(X_train)\n",
    "hidden_states = hmm_model.predict(X_train)\n",
    "\n",
    "df_regimes = pd.DataFrame({\n",
    "    \"date\": data.index[:len(hidden_states)],\n",
    "    \"close\": data[\"Close\"].iloc[:len(hidden_states)].values,\n",
    "    \"regime\": hidden_states\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "for regime in np.unique(df_regimes[\"regime\"]):\n",
    "    mask = df_regimes[\"regime\"] == regime\n",
    "    ax.plot(df_regimes[\"date\"][mask], df_regimes[\"close\"][mask], '.', label=f\"Regime {regime}\")\n",
    "\n",
    "ax.set_title(\"Market Regimes Detected by HMM\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Close Price\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i, (mean, cov) in enumerate(zip(hmm_model.means_, hmm_model.covars_)):\n",
    "    print(f\"Regime {i}\")\n",
    "    print(\"Mean vector:\", mean)\n",
    "    print(\"Volatility proxy (diag(cov)):\", np.sqrt(np.diag(cov)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb591b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Regiemem implementation\n",
    "\"\"\"\n",
    "x_data = create_features(data)\n",
    "feature_cols = [col for col in train_data.columns if any(key in col for key in ['Log_Return', 'Volatility', 'Momentum', 'Z_'])]\n",
    "x_data = x_data[feature_cols]\n",
    "\n",
    "x = scaler.transform(x_data)\n",
    "\n",
    "x = pca.transform(x)\n",
    "probs = hmm_model.predict_proba(x)\n",
    "\n",
    "regime_confidence = probs.max(axis=1)\n",
    "most_likely_regime = probs.argmax(axis=1)\n",
    "\n",
    "x_data[\"Regime_Prob\"] = regime_confidence\n",
    "x_data[\"Most_Likely_Regime\"] = most_likely_regime\n",
    "x_data[\"Most_Likely_Regime\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    General ML Regieme Detection MOdel (General_Model)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class General_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "gen_model = General_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Asset Specific(Residual_Model_A) ML Regieme Detection MOdel\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class Residual_Model_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "res_model = Residual_Model_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "     final output General_Model(x) + Residual_Model_A(x)\n",
    "\"\"\"\n",
    "\n",
    "out = gen_model(x) + res_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54686e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Monte arlo simm stress Test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a4f7c",
   "metadata": {},
   "source": [
    "                     +-------------------+\n",
    "                     | Raw Market Data   |\n",
    "                     | (multi-asset)     |\n",
    "                     +--------+----------+\n",
    "                              |\n",
    "                     +--------v----------+\n",
    "                     | Feature Generator |\n",
    "                     | - Rolling stats   |\n",
    "                     | - Regime probs    |\n",
    "                     +--------+----------+\n",
    "                              |\n",
    "               +--------------v-------------+\n",
    "               | General Market State Model |\n",
    "               | (Trained on pooled data)   |\n",
    "               +--------------+-------------+\n",
    "                              |\n",
    "        +---------------------v---------------------+\n",
    "        | For each asset A:                         |\n",
    "        | - Compute residuals                       |\n",
    "        | - Train Residual_Model_A                  |\n",
    "        |   (on asset-specific patterns)            |\n",
    "        +---------------------+---------------------+\n",
    "                              |\n",
    "            +-----------------v------------------+\n",
    "            | Final Prediction for asset A:       |\n",
    "            | General_Model(x) + Residual_Model_A(x) |\n",
    "            +-----------------+------------------+\n",
    "\n",
    "                          ↓\n",
    "          +------------------------------------+\n",
    "          | Monte Carlo Simulation / Scenarios |\n",
    "          | (for stress-testing and robustness)|\n",
    "          +------------------------------------+\n",
    "\n",
    "                          ↓\n",
    "          +----------------+------------------+\n",
    "          | Evaluation & Metrics               |\n",
    "          | - Asset-level & portfolio-level    |\n",
    "          +------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651851fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hmmlearn pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hmm\n",
    "\"\"\"\n",
    "import os\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# ------------------ Config ------------------\n",
    "SYMBOLS = [\"AAPL\", \"MSFT\", \"GOOG\", \"META\", \"AMZN\"]\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = \"2025-06-27\"\n",
    "PERIOD = \"1d\"\n",
    "LOOKBACKS = [1, 5, 10, 21]\n",
    "WINDOW_SIZE = 20\n",
    "HMM_COMPONENTS = 5\n",
    "PCA_COMPONENTS = 5\n",
    "DATA_DIR = \"multi_asset_hmm\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------ Feature Generator ------------------\n",
    "def create_features(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    for days in LOOKBACKS:\n",
    "        df[f\"Log_Return_{days}\"] = np.log(df['Close'] / df['Close'].shift(days))\n",
    "        if (LOOKBACKS.index(days) + 1) % 2 == 0:\n",
    "            df[f'Volatility_{days}'] = df[f\"Log_Return_{days}\"].rolling(days, min_periods=1).std()\n",
    "        if days >= 10:\n",
    "            df[f\"Momentum_{days}\"] = df[\"Close\"].shift(1) - df[\"Close\"].shift(days + 1)\n",
    "\n",
    "    log_volume = np.log(df[\"Volume\"].shift(1) + 1e-6)\n",
    "    rolling_mean = log_volume.rolling(window=5, min_periods=5).mean()\n",
    "    rolling_std = log_volume.rolling(window=5, min_periods=5).std()\n",
    "    df[\"Z_Log_Volume\"] = (log_volume - rolling_mean) / (rolling_std + 1e-6)\n",
    "\n",
    "    ma = df[\"Close\"].rolling(WINDOW_SIZE).mean()\n",
    "    std = df[\"Close\"].rolling(WINDOW_SIZE).std()\n",
    "    df[\"Z_Price_vs_MA\"] = (df[\"Close\"] - ma) / (std + 1e-6)\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "# ------------------ HMM Training Function ------------------\n",
    "def train_asset_hmm(symbol):\n",
    "    print(f\"\\n--- {symbol} ---\")\n",
    "\n",
    "    data = yf.Ticker(symbol).history(start=START_DATE, end=END_DATE, interval=PERIOD)\n",
    "    data = data.drop([\"Dividends\", \"Stock Splits\"], axis=1)\n",
    "    df = create_features(data)\n",
    "\n",
    "    feature_cols = [col for col in df.columns if any(key in col for key in ['Log_Return', 'Volatility', 'Momentum', 'Z_'])]\n",
    "    X = df[feature_cols]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA(n_components=PCA_COMPONENTS)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    hmm_model = GaussianHMM(\n",
    "        n_components=HMM_COMPONENTS,\n",
    "        covariance_type=\"full\",\n",
    "        n_iter=400,\n",
    "        tol=1e-4,\n",
    "        verbose=False,\n",
    "        init_params=\"stmc\"\n",
    "    )\n",
    "    hmm_model.fit(X_pca)\n",
    "\n",
    "    regime_probs = hmm_model.predict_proba(X_pca)\n",
    "    most_likely = np.argmax(regime_probs, axis=1)\n",
    "    confidence = regime_probs[np.arange(len(most_likely)), most_likely]\n",
    "\n",
    "    df[\"Most_Likely_Regime\"] = most_likely\n",
    "    df[\"Regime_Prob\"] = confidence\n",
    "\n",
    "    #plot_regimes(df, symbol)\n",
    "\n",
    "    return {\n",
    "        \"symbol\": symbol,\n",
    "        \"model\": hmm_model,\n",
    "        \"features\": df,\n",
    "        \"pca\": pca,\n",
    "        \"scaler\": scaler\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------ Plotting ------------------\n",
    "def plot_regimes(df, symbol):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for regime in sorted(df[\"Regime\"].unique()):\n",
    "        mask = df[\"Regime\"] == regime\n",
    "        plt.plot(df.index[mask], df[\"Close\"][mask], \".\", label=f\"Regime {regime}\")\n",
    "    plt.title(f\"{symbol}: Regimes Detected by HMM\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Close Price\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DATA_DIR, f\"{symbol}_regimes.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# ------------------ Main Loop ------------------\n",
    "asset_models = {}\n",
    "\n",
    "for symbol in SYMBOLS:\n",
    "    try:\n",
    "        result = train_asset_hmm(symbol)\n",
    "        asset_models[symbol] = result\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {symbol}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb591b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Regiemem implementation\n",
    "\"\"\"\n",
    "extended_dfs = {}\n",
    "\n",
    "def extend_features(data, symbol, return_horizon=5, vol_multiplier=0.75):\n",
    "    df = data.copy()\n",
    "\n",
    "    df[\"Forward_Return_5\"] = df[\"Close\"].shift(-return_horizon) / df[\"Close\"] - 1\n",
    "    df[\"Forward_Log_Return_5\"] = np.log(df[\"Close\"].shift(-return_horizon) / df[\"Close\"])\n",
    "\n",
    "    regime_mult_map = {\n",
    "        0: 0.5,\n",
    "        1: 0.75,\n",
    "        2: 1.0,\n",
    "        3: 1.25,\n",
    "        4: 1.5\n",
    "    }\n",
    "    df[\"Regime_Multiplier\"] = df[\"Most_Likely_Regime\"].map(regime_mult_map)\n",
    "    df[\"Vol_Adj_Threshold\"] = df[\"Volatility_5\"] * df[\"Regime_Multiplier\"].fillna(1.0)\n",
    "\n",
    "    def label_vol_based(row):\n",
    "        if row[\"Forward_Return_5\"] > row[\"Vol_Adj_Threshold\"]:\n",
    "            return \"Bull\"\n",
    "        elif row[\"Forward_Return_5\"] < -row[\"Vol_Adj_Threshold\"]:\n",
    "            return \"Bear\"\n",
    "        else:\n",
    "            return \"Neutral\"\n",
    "\n",
    "    df[\"Market_State_Threshold\"] = df.apply(label_vol_based, axis=1)\n",
    "\n",
    "    rolling_mean = df[\"Forward_Return_5\"].rolling(LOOKBACKS[-1]).mean()\n",
    "    rolling_std = df[\"Forward_Return_5\"].rolling(LOOKBACKS[-1]).std()\n",
    "    df[\"Z_Score\"] = (df[\"Forward_Return_5\"] - rolling_mean) / rolling_std\n",
    "\n",
    "    df[\"Asset\"] = SYMBOLS.index(symbol)\n",
    "\n",
    "    df[\"Market_State_ZScore\"] = pd.cut(\n",
    "        df[\"Z_Score\"],\n",
    "        bins=[-np.inf, -0.5, 0.5, np.inf],\n",
    "        labels=[0, 1, 2]\n",
    "    )\n",
    "    df = df.drop([\"Z_Score\", \"Market_State_Threshold\", \"Forward_Return_5\", \"Forward_Log_Return_5\",\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Vol_Adj_Threshold\",], axis=1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    extended_dfs[symbol] = df\n",
    "\n",
    "    return df\n",
    "\n",
    "general_data_features = []\n",
    "for symbol in SYMBOLS:\n",
    "  general_data_features.append(extend_features(asset_models[symbol][\"features\"], symbol))\n",
    "\n",
    "general_data = pd.concat(general_data_features, axis=0)\n",
    "general_data = general_data.sort_index()\n",
    "general_data[\"Market_State_ZScore\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    General ML Regieme Detection MOdel (General_Model)\n",
    "\"\"\"\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.layers import LSTM, Dense, Layer, Input, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import layers\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ------------------ Config ------------------\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "MODEL_SHAPES = [16, 16]\n",
    "# n_components = \n",
    "# --------------------------------------------\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.dense = Dense(1, activation='tanh')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_weights = tf.nn.softmax(self.dense(inputs), axis=1)\n",
    "        context_vector = inputs * attention_weights\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "def create_sequences(x, y, seq_length=16):\n",
    "    xs, ys = [], []\n",
    "    x_values = x\n",
    "    y_values = y\n",
    "    for i in range(len(x_values) - seq_length):\n",
    "        xs.append(x_values[i:i+seq_length])\n",
    "        ys.append(y_values[i+seq_length])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "class General_Model:\n",
    "  def __init__(self, general_data, seq_length=16, model_type=\"nn\"):\n",
    "\n",
    "    self.seq_length = seq_length\n",
    "    self.feature_cols = general_data.columns.difference([\"Market_State_ZScore\"])\n",
    "    gen_model_x = general_data[self.feature_cols]\n",
    "    gen_model_y = general_data[\"Market_State_ZScore\"]\n",
    "\n",
    "    if hasattr(gen_model_y, 'cat'):\n",
    "        gen_model_y = gen_model_y.cat.codes\n",
    "    x_train, x_test, y_train, y_test = train_test_split(gen_model_x, gen_model_y, test_size=0.15, shuffle=True)\n",
    "\n",
    "    if model_type == \"nn\":\n",
    "\n",
    "      gen_model_scaler = StandardScaler()\n",
    "      x_train = gen_model_scaler.fit_transform(x_train)\n",
    "      x_test = gen_model_scaler.transform(x_test)\n",
    "\n",
    "      # gen_model_pca = PCA(n_components=n_components)\n",
    "      # x_train = gen_model_pca.fit_transform(x_train)\n",
    "      # x_test = gen_model_pca.transform(x_test)\n",
    "\n",
    "      self.gen_x_train, self.gen_y_train = create_sequences(x_train, y_train, seq_length)\n",
    "      self.gen_x_test, self.gen_y_test = create_sequences(x_test, y_test, seq_length)\n",
    "\n",
    "    elif model_type == \"xgb\":\n",
    "      self.gen_x_train, self.gen_y_train = x_train, y_train\n",
    "      self.gen_x_test, self.gen_y_test = x_test, y_test\n",
    "\n",
    "  def xgb_init(self):\n",
    "    self.xgb_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(\n",
    "          objective='multi:softprob',\n",
    "          num_class=3,\n",
    "          n_estimators=1500,\n",
    "          max_depth=2,\n",
    "          learning_rate=LEARNING_RATE,\n",
    "          subsample=0.8,\n",
    "          colsample_bytree=1,\n",
    "          min_child_weight=3,\n",
    "          gamma=1,\n",
    "          reg_alpha=0.5,\n",
    "          reg_lambda=1.0,\n",
    "          verbosity=2,\n",
    "          use_label_encoder=False,\n",
    "          eval_metric=\"mlogloss\"\n",
    "      ))\n",
    "    ])\n",
    "\n",
    "  def train_xgb(self):\n",
    "    self.xgb_init()\n",
    "    self.xgb_pipeline.fit(self.gen_x_train, self.gen_y_train)\n",
    "\n",
    "    self.gen_y_pred = self.xgb_pipeline.predict(self.gen_x_test)\n",
    "\n",
    "    acc = accuracy_score(self.gen_y_test, self.gen_y_pred)\n",
    "\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    return self.xgb_pipeline\n",
    "\n",
    "  def init__nn(self, input_shape, lr = 1e-2):\n",
    "    def build_lstm_model(input_shape, units=64):\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        lstm_out = Bidirectional(LSTM(units, return_sequences=True))(inputs)\n",
    "        lstm_out = Bidirectional(LSTM(units*2, return_sequences=True))(lstm_out)\n",
    "\n",
    "        attention_out = AttentionLayer()(lstm_out)\n",
    "\n",
    "        outputs = Dense(3, activation='softmax')(attention_out)\n",
    "        return Model(inputs, outputs)\n",
    "\n",
    "    input_shape = (input_shape, len(self.feature_cols))\n",
    "\n",
    "    experts = []\n",
    "    for units in MODEL_SHAPES:\n",
    "      experts.append(build_lstm_model(input_shape, units=units))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    expert_outputs = []\n",
    "    for expert in experts:\n",
    "      out = expert(inputs)\n",
    "      expert_outputs.append(out)\n",
    "\n",
    "    combined = Concatenate()(expert_outputs)\n",
    "    outputs = Dense(3, activation='softmax')(combined)\n",
    "    self.gen_ensemble_model = Model(inputs, outputs)\n",
    "    self.gen_ensemble_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                          loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  def train__nn(self, epochs=50, batch_size=16, lr = 1e-2):\n",
    "    self.init__nn(input_shape=batch_size, lr = lr)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    self.gen_ensemble_model.fit(\n",
    "        self.gen_x_train, self.gen_y_train,\n",
    "        validation_data=(self.gen_x_test, self.gen_y_test),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        #callbacks=[reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    self.gen_ensemble_model.evaluate(\n",
    "        self.gen_x_test, self.gen_y_test,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1\n",
    "    )\n",
    "    return self.gen_ensemble_model\n",
    "\n",
    "  def plot_xgb(self):\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(self.gen_y_test, self.gen_y_pred))\n",
    "\n",
    "    cm = confusion_matrix(self.gen_y_test, self.gen_y_pred)\n",
    "    labels = self.gen_y_test.unique()\n",
    "    plt.figure(figsize=(6,5))\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "gem_model = General_Model(general_data, seq_length=BATCH_SIZE)\n",
    "# xgb_pipeline = gem_model.train_xgb()\n",
    "ensemble_model = gem_model.train__nn(epochs=EPOCHS, batch_size=BATCH_SIZE, lr = LEARNING_RATE)\n",
    "# gem_model.plot_xgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Asset Specific(Residual_Model_A) ML Regieme Detection MOdel\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class Residual_Model_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "res_model = Residual_Model_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "     final output General_Model(x) + Residual_Model_A(x)\n",
    "\"\"\"\n",
    "\n",
    "out = gen_model(x) + res_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54686e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Monte arlo simm stress Test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a4f7c",
   "metadata": {},
   "source": [
    "                     +-------------------+\n",
    "                     | Raw Market Data   |\n",
    "                     | (multi-asset)     |\n",
    "                     +--------+----------+\n",
    "                              |\n",
    "                     +--------v----------+\n",
    "                     | Feature Generator |\n",
    "                     | - Rolling stats   |\n",
    "                     | - Regime probs    |\n",
    "                     +--------+----------+\n",
    "                              |\n",
    "               +--------------v-------------+\n",
    "               | General Market State Model |\n",
    "               | (Trained on pooled data)   |\n",
    "               +--------------+-------------+\n",
    "                              |\n",
    "        +---------------------v---------------------+\n",
    "        | For each asset A:                         |\n",
    "        | - Compute residuals                       |\n",
    "        | - Train Residual_Model_A                  |\n",
    "        |   (on asset-specific patterns)            |\n",
    "        +---------------------+---------------------+\n",
    "                              |\n",
    "            +-----------------v------------------+\n",
    "            | Final Prediction for asset A:       |\n",
    "            | General_Model(x) + Residual_Model_A(x) |\n",
    "            +-----------------+------------------+\n",
    "\n",
    "                          ↓\n",
    "          +------------------------------------+\n",
    "          | Monte Carlo Simulation / Scenarios |\n",
    "          | (for stress-testing and robustness)|\n",
    "          +------------------------------------+\n",
    "\n",
    "                          ↓\n",
    "          +----------------+------------------+\n",
    "          | Evaluation & Metrics               |\n",
    "          | - Asset-level & portfolio-level    |\n",
    "          +------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651851fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
